---
title: OSX Heap
date: 2019-07-19
---

### Malloc

애플에서 libmalloc을 오픈 해놨기 때문에 소스를 같이 보면 좋을 듯 하다! [source](https://opensource.apple.com/source/libmalloc/libmalloc-166.220.1/src/)

malloc_zone_malloc 함수에서 default_zone 전역 변수를 사용한다.

```c
# malloc in libmalloc.dylib

void *
malloc(size_t size)
{
    void *retval;
    retval = malloc_zone_malloc(default_zone, size);
    if (retval == NULL) {
        errno = ENOMEM;
    }
    return retval;
}
```



OSX/ios 에서는 zone을 사용하게 되는데 하나의 zone이 하나의 heap 페이징이라고 볼 수 있을듯 하다. 해당 영역이 파괴되면 구역 내에 할당된 모든 블록들은 다 free가 된다. zone은 유사한 속성을 가진 블록을 함께 배치할 수 있도록 한다. 

zone에 대한 구조는 아래와 같다. 

```c
#/usr/include/malloc.h

typedef struct _malloc_zone_t {
    /* Only zone implementors should depend on the layout of this structure; Regular callers should use the access functions below */
    void	*reserved1;	/* RESERVED FOR CFAllocator DO NOT USE */
    void	*reserved2;	/* RESERVED FOR CFAllocator DO NOT USE */
    size_t 	(* MALLOC_ZONE_FN_PTR(size))(struct _malloc_zone_t *zone, const void *ptr); /* returns the size of a block or 0 if not in this zone; must be fast, especially for negative answers */
    void 	*(* MALLOC_ZONE_FN_PTR(malloc))(struct _malloc_zone_t *zone, size_t size);
    void 	*(* MALLOC_ZONE_FN_PTR(calloc))(struct _malloc_zone_t *zone, size_t num_items, size_t size); /* same as malloc, but block returned is set to zero */
    void 	*(* MALLOC_ZONE_FN_PTR(valloc))(struct _malloc_zone_t *zone, size_t size); /* same as malloc, but block returned is set to zero and is guaranteed to be page aligned */
    void 	(* MALLOC_ZONE_FN_PTR(free))(struct _malloc_zone_t *zone, void *ptr);
    void 	*(* MALLOC_ZONE_FN_PTR(realloc))(struct _malloc_zone_t *zone, void *ptr, size_t size);
    void 	(* MALLOC_ZONE_FN_PTR(destroy))(struct _malloc_zone_t *zone); /* zone is destroyed and all memory reclaimed */
    const char	*zone_name;

    /* Optional batch callbacks; these may be NULL */
    unsigned	(* MALLOC_ZONE_FN_PTR(batch_malloc))(struct _malloc_zone_t *zone, size_t size, void **results, unsigned num_requested); /* given a size, returns pointers capable of holding that size; returns the number of pointers allocated (maybe 0 or less than num_requested) */
    void	(* MALLOC_ZONE_FN_PTR(batch_free))(struct _malloc_zone_t *zone, void **to_be_freed, unsigned num_to_be_freed); /* frees all the pointers in to_be_freed; note that to_be_freed may be overwritten during the process */

    struct malloc_introspection_t	* MALLOC_INTROSPECT_TBL_PTR(introspect);
    unsigned	version;
    	
    /* aligned memory allocation. The callback may be NULL. Present in version >= 5. */
    void *(* MALLOC_ZONE_FN_PTR(memalign))(struct _malloc_zone_t *zone, size_t alignment, size_t size);
    
    /* free a pointer known to be in zone and known to have the given size. The callback may be NULL. Present in version >= 6.*/
    void (* MALLOC_ZONE_FN_PTR(free_definite_size))(struct _malloc_zone_t *zone, void *ptr, size_t size);

    /* Empty out caches in the face of memory pressure. The callback may be NULL. Present in version >= 8. */
    size_t 	(* MALLOC_ZONE_FN_PTR(pressure_relief))(struct _malloc_zone_t *zone, size_t goal);

	/* * Checks whether an address might belong to the zone. May be NULL. Present in version >= 10. * False positives are allowed (e.g. the pointer was freed, or it's in zone space that has * not yet been allocated. False negatives are not allowed. */
    boolean_t (* MALLOC_ZONE_FN_PTR(claimed_address))(struct _malloc_zone_t *zone, void *ptr);
} malloc_zone_t;
```



default_zone 전역 변수에 대한 구조는 다음과 같다.  여기서 default_zone포인터를 virtual_default_zone으로 명시 했는데 virtual에 걸맞게 거짓 zone이라고 한다.

```c
# malloc.c

typedef struct {
	malloc_zone_t malloc_zone;
	uint8_t pad[PAGE_MAX_SIZE - sizeof(malloc_zone_t)];
} virtual_default_zone_t;

static virtual_default_zone_t virtual_default_zone
__attribute__((section("__DATA,__v_zone")))
__attribute__((aligned(PAGE_MAX_SIZE))) = {
	NULL,
	NULL,
	default_zone_size,
	default_zone_malloc,
	default_zone_calloc,
	default_zone_valloc,
	default_zone_free,
	default_zone_realloc,
	default_zone_destroy,
	DEFAULT_MALLOC_ZONE_STRING,
	default_zone_batch_malloc,
	default_zone_batch_free,
	&default_zone_introspect,
	10,
	default_zone_memalign,
	default_zone_free_definite_size,
	default_zone_pressure_relief,
	default_zone_malloc_claimed_address,
};

static malloc_zone_t *default_zone = &virtual_default_zone.malloc_zone;
```



다음은 malloc_zone_malloc 함수 호출이다. 여기서의 ptr = zone->malloc(zone, size);는 default_zone_malloc 함수이다.

```c
# malloc.c

void *
malloc_zone_malloc(malloc_zone_t *zone, size_t size)
{
	MALLOC_TRACE(TRACE_malloc | DBG_FUNC_START, (uintptr_t)zone, size, 0, 0);

	void *ptr;
	if (malloc_check_start && (malloc_check_counter++ >= malloc_check_start)) {
		internal_check();
	}
	if (size > MALLOC_ABSOLUTE_MAX_SIZE) {
		return NULL;
	}

	ptr = zone->malloc(zone, size);		// if lite zone is passed in then we still call the lite methods

	
	if (malloc_logger) {
		malloc_logger(MALLOC_LOG_TYPE_ALLOCATE | MALLOC_LOG_TYPE_HAS_ZONE, (uintptr_t)zone, (uintptr_t)size, 0, (uintptr_t)ptr, 0);
	}

	MALLOC_TRACE(TRACE_malloc | DBG_FUNC_END, (uintptr_t)zone, size, (uintptr_t)ptr, 0);
	return ptr;
}
```



이 코드가 진짜배기인데 여기가 real zone, 런타임과 런타임이 아닐 때 디버깅 걸어서 분석했을 때 서로 zone이 다른 이유가 이것 인것 같기도.. 

```c
static void *
default_zone_malloc(malloc_zone_t *zone, size_t size)
{
    zone = runtime_default_zone();

    return zone->malloc(zone, size);
}
```

```c
runtime_default_zone   // inline
----inline_malloc_default_zone  //inline
--------_malloc_initialize_once  //inline 
------------_malloc_initialize()
```



실제 zone안에 데이터들을 설정하는 함수 인듯하다..

```c
# malloc.c

static void
_malloc_initialize(void *context __unused)
{
	MALLOC_LOCK();
	unsigned n;
	malloc_zone_t *zone = NULL;

	if (!_malloc_entropy_initialized) {
		// Lazy initialization may occur before __malloc_init (rdar://27075409)
		// TODO: make this a fatal error
		malloc_report(ASL_LEVEL_ERR, "*** malloc was initialized without entropy\n");
	}

	phys_ncpus = *(uint8_t *)(uintptr_t)_COMM_PAGE_PHYSICAL_CPUS;
	logical_ncpus = *(uint8_t *)(uintptr_t)_COMM_PAGE_LOGICAL_CPUS;

	if (0 != (logical_ncpus % phys_ncpus)) {
		MALLOC_REPORT_FATAL_ERROR(logical_ncpus % phys_ncpus,
				"logical_ncpus %% phys_ncpus != 0\n");
	}

	switch (logical_ncpus / phys_ncpus) {
	case 1:
		hyper_shift = 0;
		break;
	case 2:
		hyper_shift = 1;
		break;
	case 4:
		hyper_shift = 2;
		break;
	default:
		MALLOC_REPORT_FATAL_ERROR(logical_ncpus / phys_ncpus, "logical_ncpus / phys_ncpus not 1, 2, or 4");
	}

	// max_magazines may already be set from a boot argument. Make sure that it
	// is bounded by the number of CPUs.
	if (max_magazines) {
		max_magazines = MIN(max_magazines, logical_ncpus);
	} else {
		max_magazines = logical_ncpus;
	}

	set_flags_from_environment(); // will only set flags up to two times
	n = malloc_num_zones;

#if CONFIG_NANOZONE
	nano_common_configure();
	
	malloc_zone_t *helper_zone = create_scalable_zone(0, malloc_debug_flags);

	if (_malloc_engaged_nano == NANO_V2) {
		zone = nanov2_create_zone(helper_zone, malloc_debug_flags);
	} else if (_malloc_engaged_nano == NANO_V1) {
		zone = nano_create_zone(helper_zone, malloc_debug_flags);
	}

	if (zone) {
		malloc_zone_register_while_locked(zone);
		malloc_zone_register_while_locked(helper_zone);

		// Must call malloc_set_zone_name() *after* helper and nano are hooked together.
		malloc_set_zone_name(zone, DEFAULT_MALLOC_ZONE_STRING);
		malloc_set_zone_name(helper_zone, MALLOC_HELPER_ZONE_STRING);
	} else {
		zone = helper_zone;
		malloc_zone_register_while_locked(zone);
		malloc_set_zone_name(zone, DEFAULT_MALLOC_ZONE_STRING);
	}
#else
	zone = create_scalable_zone(0, malloc_debug_flags);
	malloc_zone_register_while_locked(zone);
	malloc_set_zone_name(zone, DEFAULT_MALLOC_ZONE_STRING);
#endif

	initial_default_zone = zone;

	if (n != 0) { // make the default first, for efficiency
		unsigned protect_size = malloc_num_zones_allocated * sizeof(malloc_zone_t *);
		malloc_zone_t *hold = malloc_zones[0];

		if (hold->zone_name && strcmp(hold->zone_name, DEFAULT_MALLOC_ZONE_STRING) == 0) {
			malloc_set_zone_name(hold, NULL);
		}

		mprotect(malloc_zones, protect_size, PROT_READ | PROT_WRITE);
		malloc_zones[0] = malloc_zones[n];
		malloc_zones[n] = hold;
		mprotect(malloc_zones, protect_size, PROT_READ);
	}

	// Only setup stack logging hooks once lazy initialization is complete, the
	// malloc_zone calls above would otherwise initialize malloc stack logging,
	// which calls into malloc re-entrantly from Libc upcalls and so deadlocks
	// in the lazy initialization os_once(). rdar://13046853
	if (stack_logging_enable_logging) {
		switch (stack_logging_mode) {
		case stack_logging_mode_malloc:
			malloc_logger = __disk_stack_logging_log_stack;
			break;
		case stack_logging_mode_vm:
			__syscall_logger = __disk_stack_logging_log_stack;
			break;
		case stack_logging_mode_all:
			malloc_logger = __disk_stack_logging_log_stack;
			__syscall_logger = __disk_stack_logging_log_stack;
			break;
		case stack_logging_mode_lite:
			__syscall_logger = __disk_stack_logging_log_stack;
			create_and_insert_lite_zone_while_locked();
			enable_stack_logging_lite();
			break;
		case stack_logging_mode_vmlite:
			__syscall_logger = __disk_stack_logging_log_stack;
			break;
		}
	}

	// malloc_report(ASL_LEVEL_INFO, "%d registered zones\n", malloc_num_zones);
	// malloc_report(ASL_LEVEL_INFO, "malloc_zones is at %p; malloc_num_zones is at %p\n", (unsigned)&malloc_zones,
	// (unsigned)&malloc_num_zones);
	MALLOC_UNLOCK();
}

```

해당 함수에서 zone에 permission이 변경되는 것을 볼 수 있다.

```c
mprotect(malloc_zones, protect_size, PROT_READ | PROT_WRITE);
		malloc_zones[0] = malloc_zones[n];
		malloc_zones[n] = hold;
		mprotect(malloc_zones, protect_size, PROT_READ);
```



실제 default_zone 은 scalable_zone 이다!

```c
# magazine_malloc.c

malloc_zone_t *
create_scalable_zone(size_t initial_size, unsigned debug_flags) {
	return (malloc_zone_t *) create_scalable_szone(initial_size, debug_flags);
}
```



그리고 위에서 호출되는 create_scalable_szone 함수이다. malloc, free 등의 함수를 실제로 세팅하는 듯 하다. 솔직히 넘 길어서 알기가 힘들다..

```c
# magazine_malloc.c

szone_t *
create_scalable_szone(size_t initial_size, unsigned debug_flags)
{
	szone_t *szone;

#if defined(__i386__) || defined(__x86_64__)
	if (_COMM_PAGE_VERSION_REQD > (*((uint16_t *)_COMM_PAGE_VERSION))) {
		MALLOC_REPORT_FATAL_ERROR((*((uint16_t *)_COMM_PAGE_VERSION)), "comm page version mismatch");
	}
#endif

	/* get memory for the zone. */
	szone = mvm_allocate_pages(SZONE_PAGED_SIZE, 0, 0, VM_MEMORY_MALLOC);
	if (!szone) {
		return NULL;
	}

/* set up the szone structure */
#if 0
#warning CHECK_REGIONS enabled
	debug_flags |= CHECK_REGIONS;
#endif
#if 0
#warning LOG enabled
	szone->log_address = ~0;
#endif

	if (mvm_aslr_enabled()) {
		debug_flags &= ~DISABLE_ASLR;
	} else {
		debug_flags |= DISABLE_ASLR;
	}

#if CONFIG_SMALL_CUTOFF_DYNAMIC || CONFIG_LARGE_CACHE
	uint64_t memsize = platform_hw_memsize();
#endif

	bool is_largemem = false;
#if CONFIG_SMALL_CUTOFF_LARGEMEM
	is_largemem = true;
#elif CONFIG_SMALL_CUTOFF_DYNAMIC
	// TODO: rdar://problem/35395572
	// switch to largemem thresholds on devices with > 2 cores and > 2gb of memory
	uint32_t nproc = platform_cpu_count();
	is_largemem = (nproc > 2) && (memsize > (2ull << 30));
#endif
	if (is_largemem) {
		debug_flags |= MALLOC_EXTENDED_SMALL_SLOTS;
		szone->is_largemem = 1;
		szone->large_threshold = LARGE_THRESHOLD_LARGEMEM;
		szone->vm_copy_threshold = VM_COPY_THRESHOLD_LARGEMEM;
	} else {
		debug_flags &= ~MALLOC_EXTENDED_SMALL_SLOTS;
		szone->is_largemem = 0;
		szone->large_threshold = LARGE_THRESHOLD;
		szone->vm_copy_threshold = VM_COPY_THRESHOLD;
	}

	// Query the number of configured processors.
	// Uniprocessor case gets just one tiny and one small magazine (whose index is zero). This gives
	// the same behavior as the original scalable malloc. MP gets per-CPU magazines
	// that scale (way) better.
	unsigned int max_mags = mag_max_magazines();
	uint32_t num_magazines = (max_mags > 1) ? MIN(max_mags, TINY_MAX_MAGAZINES) : 1;
	rack_init(&szone->tiny_rack, RACK_TYPE_TINY, num_magazines, debug_flags);
	rack_init(&szone->small_rack, RACK_TYPE_SMALL, num_magazines, debug_flags);

#if CONFIG_LARGE_CACHE
	// madvise(..., MADV_REUSABLE) death-row arrivals above this threshold [~0.1%]
	szone->large_entry_cache_reserve_limit = (size_t)(memsize >> 10);

	/* <rdar://problem/6610904> Reset protection when returning a previous large allocation? */
	int32_t libSystemVersion = NSVersionOfLinkTimeLibrary("System");
	if ((-1 != libSystemVersion) && ((libSystemVersion >> 16) < 112) /* CFSystemVersionSnowLeopard */) {
		szone->large_legacy_reset_mprotect = TRUE;
	} else {
		szone->large_legacy_reset_mprotect = FALSE;
	}
#endif

	// Initialize the security token.
	szone->cookie = (uintptr_t)malloc_entropy[0];

	szone->basic_zone.version = 10;
	szone->basic_zone.size = (void *)szone_size;
	szone->basic_zone.malloc = (void *)szone_malloc;
	szone->basic_zone.calloc = (void *)szone_calloc;
	szone->basic_zone.valloc = (void *)szone_valloc;
	szone->basic_zone.free = (void *)szone_free;
	szone->basic_zone.realloc = (void *)szone_realloc;
	szone->basic_zone.destroy = (void *)szone_destroy;
	szone->basic_zone.batch_malloc = (void *)szone_batch_malloc;
	szone->basic_zone.batch_free = (void *)szone_batch_free;
	szone->basic_zone.introspect = (struct malloc_introspection_t *)&szone_introspect;
	szone->basic_zone.memalign = (void *)szone_memalign;
	szone->basic_zone.free_definite_size = (void *)szone_free_definite_size;
	szone->basic_zone.pressure_relief = (void *)szone_pressure_relief;
	szone->basic_zone.claimed_address = (void *)szone_claimed_address;

	/* Set to zero once and for all as required by CFAllocator. */
	szone->basic_zone.reserved1 = 0;
	/* Set to zero once and for all as required by CFAllocator. */
	szone->basic_zone.reserved2 = 0;

	/* Prevent overwriting the function pointers in basic_zone. */
	mprotect(szone, sizeof(szone->basic_zone), PROT_READ);

	szone->debug_flags = debug_flags;
	_malloc_lock_init(&szone->large_szone_lock);

	szone->cpu_id_key = -1UL; // Unused.

	CHECK(szone, __PRETTY_FUNCTION__);
	return szone;
}
```

바인딩 순서를 살펴보면 malloc_zone_malloc -> default_zone_malloc -> malloc_initialize -> create_scalable_szone 일듯 하다. 바인딩 되는거 보려고했는데 lldb로 못보겠다..ㅠㅠ



### Malloc Size

ptmalloc2에서 size에 맞춰 fast chunk, small chunk, large chunk가 나뉘듯이 OSX도 size별로 rack이 나뉘어져 있다.

##### Tiny

- 1008 byte > size (64bit)
- 496 byte > size (32bit)

##### Small

- 15KB > size > Tiny (if less 1gb memory)
- 127KB > size > Tiny 

##### Large

- size > Small



### Magazine

malloc zone 안에 magazine이라는 것이 존재한다.

일단 zone안에 rack을 할당했을 때 rack 내부에 magazine이 존재하며 magazine 안에 region이 존재하며 할당 size에 따라 region이 구분되는 듯 하다. 각 region은 quantum으로 세분화 되며 각 rack 별 metadata는 region 끝에 존재한다. allocation 자체는 n 개의 quantum으로 이루어져있는 하나의 block이다. 

여러 개의 프로세스가 있으면 rack은 여러개의 magazine을 가지고 있다. 

##### 

tiny, small, large가 여기서 나뉘는 듯

```c
MALLOC_NOINLINE void *
szone_malloc_should_clear(szone_t *szone, size_t size, boolean_t cleared_requested)
{
    void *ptr;
    msize_t msize;

    if (size <= SMALL_THRESHOLD) {
        // tiny size: <=1008 bytes (64-bit), <=496 bytes (32-bit)
        // think tiny
        msize = TINY_MSIZE_FOR_BYTES(size + TINY_QUANTUM - 1);
        if (!msize) {
            msize = 1;
        }
        ptr = tiny_malloc_should_clear(&szone->tiny_rack, msize, cleared_requested);
    } else if (size <= szone->large_threshold) {
        // small size: <=15k (iOS), <=64k (large iOS), <=128k (macOS)
        // think small
        msize = SMALL_MSIZE_FOR_BYTES(size + SMALL_QUANTUM - 1);
        if (!msize) {
            msize = 1;
        }
        ptr = small_malloc_should_clear(&szone->small_rack, msize, cleared_requested);
    } else {
        // large: all other allocations
        size_t num_kernel_pages = round_page_quanta(size) >> vm_page_quanta_shift;
        if (num_kernel_pages == 0) { /* Overflowed */
            ptr = 0;
        } else {
            ptr = large_malloc(szone, num_kernel_pages, 0, cleared_requested);
        }
    }

    return ptr;
}
```



tiny alloc 할당 함수이다.

if (tiny_mag_ptr->mag_last_free_msize == msize) 이 조건문이 free list의 size 검증인 것 같음

ptr = tiny_malloc_from_free_list(rack, tiny_mag_ptr, mag_index, msize);

여기서 free list를 가져 오는 듯 하다..

```c
# magazine_tiny.c

void *
tiny_malloc_should_clear(rack_t *rack, msize_t msize, boolean_t cleared_requested)
{
	void *ptr;
	mag_index_t mag_index = tiny_mag_get_thread_index() % rack->num_magazines;
	magazine_t *tiny_mag_ptr = &(rack->magazines[mag_index]);

	MALLOC_TRACE(TRACE_tiny_malloc, (uintptr_t)rack, TINY_BYTES_FOR_MSIZE(msize), (uintptr_t)tiny_mag_ptr, cleared_requested);

#if DEBUG_MALLOC
	if (DEPOT_MAGAZINE_INDEX == mag_index) {
		malloc_zone_error(rack->debug_flags, true, "malloc called for magazine index -1\n");
		return (NULL);
	}

	if (!msize) {
		malloc_zone_error(rack->debug_flags, true, "invariant broken (!msize) in allocation (region)\n");
		return (NULL);
	}
#endif

	SZONE_MAGAZINE_PTR_LOCK(tiny_mag_ptr);

#if CONFIG_TINY_CACHE
	ptr = tiny_mag_ptr->mag_last_free;

	if (tiny_mag_ptr->mag_last_free_msize == msize) {
		// we have a winner
		tiny_mag_ptr->mag_last_free = NULL;
		tiny_mag_ptr->mag_last_free_msize = 0;
		tiny_mag_ptr->mag_last_free_rgn = NULL;
		SZONE_MAGAZINE_PTR_UNLOCK(tiny_mag_ptr);
		CHECK(szone, __PRETTY_FUNCTION__);
		if (cleared_requested) {
			memset(ptr, 0, TINY_BYTES_FOR_MSIZE(msize));
		}
#if DEBUG_MALLOC
		if (LOG(szone, ptr)) {
			malloc_report(ASL_LEVEL_INFO, "in tiny_malloc_should_clear(), tiny cache ptr=%p, msize=%d\n", ptr, msize);
		}
#endif
		return ptr;
	}
#endif /* CONFIG_TINY_CACHE */

	while (1) {
		ptr = tiny_malloc_from_free_list(rack, tiny_mag_ptr, mag_index, msize);
		if (ptr) {
			SZONE_MAGAZINE_PTR_UNLOCK(tiny_mag_ptr);
			CHECK(szone, __PRETTY_FUNCTION__);
			if (cleared_requested) {
				memset(ptr, 0, TINY_BYTES_FOR_MSIZE(msize));
			}
			return ptr;
		}

		if (tiny_get_region_from_depot(rack, tiny_mag_ptr, mag_index, msize)) {
			ptr = tiny_malloc_from_free_list(rack, tiny_mag_ptr, mag_index, msize);
			if (ptr) {
				SZONE_MAGAZINE_PTR_UNLOCK(tiny_mag_ptr);
				CHECK(szone, __PRETTY_FUNCTION__);
				if (cleared_requested) {
					memset(ptr, 0, TINY_BYTES_FOR_MSIZE(msize));
				}
				return ptr;
			}
		}

		// The magazine is exhausted. A new region (heap) must be allocated to satisfy this call to malloc().
		// The allocation, an mmap() system call, will be performed outside the magazine spin locks by the first
		// thread that suffers the exhaustion. That thread sets "alloc_underway" and enters a critical section.
		// Threads arriving here later are excluded from the critical section, yield the CPU, and then retry the
		// allocation. After some time the magazine is resupplied, the original thread leaves with its allocation,
		// and retry-ing threads succeed in the code just above.
		if (!tiny_mag_ptr->alloc_underway) {
			void *fresh_region;

			// time to create a new region (do this outside the magazine lock)
			tiny_mag_ptr->alloc_underway = TRUE;
			OSMemoryBarrier();
			SZONE_MAGAZINE_PTR_UNLOCK(tiny_mag_ptr);
			fresh_region = mvm_allocate_pages_securely(TINY_REGION_SIZE, TINY_BLOCKS_ALIGN, VM_MEMORY_MALLOC_TINY, rack->debug_flags);
			SZONE_MAGAZINE_PTR_LOCK(tiny_mag_ptr);

			// DTrace USDT Probe
			MAGMALLOC_ALLOCREGION(TINY_SZONE_FROM_RACK(rack), (int)mag_index, fresh_region, TINY_REGION_SIZE);

			if (!fresh_region) { // out of memory!
				tiny_mag_ptr->alloc_underway = FALSE;
				OSMemoryBarrier();
				SZONE_MAGAZINE_PTR_UNLOCK(tiny_mag_ptr);
				return NULL;
			}

			ptr = tiny_malloc_from_region_no_lock(rack, tiny_mag_ptr, mag_index, msize, fresh_region);

			// we don't clear because this freshly allocated space is pristine
			tiny_mag_ptr->alloc_underway = FALSE;
			OSMemoryBarrier();
			SZONE_MAGAZINE_PTR_UNLOCK(tiny_mag_ptr);
			CHECK(szone, __PRETTY_FUNCTION__);
			return ptr;
		} else {
			SZONE_MAGAZINE_PTR_UNLOCK(tiny_mag_ptr);
			yield();
			SZONE_MAGAZINE_PTR_LOCK(tiny_mag_ptr);
		}
	}
	/* NOTREACHED */
}
```



free 시에는 free->malloc_zone_free->szone_free 순서로 함수가 호출 된다. szone_free에 size마다 free가 정의되어 있음

```c
# magazine_malloc.c

void szone_free(szone_t *szone, void *ptr)
{
	region_t tiny_region;
	region_t small_region;

#if DEBUG_MALLOC
	if (LOG(szone, ptr)) {
		malloc_report(ASL_LEVEL_INFO, "in szone_free with %p\n", ptr);
	}
#endif
	if (!ptr) {
		return;
	}
	/*
	 * Try to free to a tiny region.
	 */
	if ((uintptr_t)ptr & (TINY_QUANTUM - 1)) {
		malloc_zone_error(szone->debug_flags, true, "Non-aligned pointer %p being freed\n", ptr);
		return;
	}
	if ((tiny_region = tiny_region_for_ptr_no_lock(&szone->tiny_rack, ptr)) != NULL) {
		if (TINY_INDEX_FOR_PTR(ptr) >= NUM_TINY_BLOCKS) {
			malloc_zone_error(szone->debug_flags, true, "Pointer %p to metadata being freed\n", ptr);
			return;
		}
		free_tiny(&szone->tiny_rack, ptr, tiny_region, 0);
		return;
	}

	/*
	 * Try to free to a small region.
	 */
	if ((uintptr_t)ptr & (SMALL_QUANTUM - 1)) {
		malloc_zone_error(szone->debug_flags, true, "Non-aligned pointer %p being freed (2)\n", ptr);
		return;
	}
	if ((small_region = small_region_for_ptr_no_lock(&szone->small_rack, ptr)) != NULL) {
		if (SMALL_META_INDEX_FOR_PTR(ptr) >= NUM_SMALL_BLOCKS) {
			malloc_zone_error(szone->debug_flags, true, "Pointer %p to metadata being freed (2)\n", ptr);
			return;
		}
		free_small(&szone->small_rack, ptr, small_region, 0);
		return;
	}

	/* check that it's a legal large allocation */
	if ((uintptr_t)ptr & (vm_page_quanta_size - 1)) {
		malloc_zone_error(szone->debug_flags, true, "non-page-aligned, non-allocated pointer %p being freed\n", ptr);
		return;
	}
	free_large(szone, ptr);
}
```



tiny free 이다.

cache라는 것을 사용하는 듯 한데 일단 free할 때 free target이 mag_last_free에 저장되는 듯 하고 mag_last_free가 NULL이 아니고 어떤 free ptr이 있다면 ptmalloc과 마찬가지로 통합해버리는 듯한데 바로는 안하는 것 같다.. 그리고 통합이 되면 메타데이터를 설정하고 해당 사이즈에 맞는 free list에 넣는 듯 하다.

```c
# magazine_tiny.c

void free_tiny(rack_t *rack, void *ptr, region_t tiny_region, size_t known_size)
{
	msize_t msize;
	boolean_t is_free;
	mag_index_t mag_index = MAGAZINE_INDEX_FOR_TINY_REGION(tiny_region);
	magazine_t *tiny_mag_ptr = &(rack->magazines[mag_index]);

	MALLOC_TRACE(TRACE_tiny_free, (uintptr_t)rack, (uintptr_t)ptr, (uintptr_t)tiny_mag_ptr, known_size);

	// ptr is known to be in tiny_region
	if (known_size) {
		msize = TINY_MSIZE_FOR_BYTES(known_size + TINY_QUANTUM - 1);
	} else {
		msize = get_tiny_meta_header(ptr, &is_free);
		if (is_free) {
			free_tiny_botch(rack, ptr);
			return;
		}
	}
#if DEBUG_MALLOC
	if (!msize) {
		malloc_report(ASL_LEVEL_ERR, "*** free_tiny() block in use is too large: %p\n", ptr);
		return;
	}
#endif

	SZONE_MAGAZINE_PTR_LOCK(tiny_mag_ptr);

#if CONFIG_TINY_CACHE
	// Depot does not participate in CONFIG_TINY_CACHE since it can't be directly malloc()'d
	if (DEPOT_MAGAZINE_INDEX != mag_index) {
		if (msize < TINY_QUANTUM) {					  // to see if the bits fit in the last 4 bits
			void *ptr2 = tiny_mag_ptr->mag_last_free; // Might be NULL
			msize_t msize2 = tiny_mag_ptr->mag_last_free_msize;
			region_t rgn2 = tiny_mag_ptr->mag_last_free_rgn;

			/* check that we don't already have this pointer in the cache */
			if (ptr == ptr2) {
				free_tiny_botch(rack, ptr);
				return;
			}

			if ((rack->debug_flags & MALLOC_DO_SCRIBBLE) && msize) {
				memset(ptr, SCRABBLE_BYTE, TINY_BYTES_FOR_MSIZE(msize));
			}

			tiny_mag_ptr->mag_last_free = ptr;
			tiny_mag_ptr->mag_last_free_msize = msize;
			tiny_mag_ptr->mag_last_free_rgn = tiny_region;

			if (!ptr2) {
				SZONE_MAGAZINE_PTR_UNLOCK(tiny_mag_ptr);
				CHECK(szone, __PRETTY_FUNCTION__);
				return;
			}

			msize = msize2;
			ptr = ptr2;
			tiny_region = rgn2;
		}
	}
#endif /* CONFIG_TINY_CACHE */

	// Now in the time it took to acquire the lock, the region may have migrated
	// from one magazine to another. I.e. trailer->mag_index is volatile.
	// In which case the magazine lock we obtained (namely magazines[mag_index].mag_lock)
	// is stale. If so, keep on tryin' ...
	region_trailer_t *trailer = REGION_TRAILER_FOR_TINY_REGION(tiny_region);
	mag_index_t refreshed_index;

	while (mag_index != (refreshed_index = trailer->mag_index)) { // Note assignment
		SZONE_MAGAZINE_PTR_UNLOCK(tiny_mag_ptr);
		mag_index = refreshed_index;
		tiny_mag_ptr = &(rack->magazines[mag_index]);
		SZONE_MAGAZINE_PTR_LOCK(tiny_mag_ptr);
	}

	if (tiny_free_no_lock(rack, tiny_mag_ptr, mag_index, tiny_region, ptr, msize)) {
		SZONE_MAGAZINE_PTR_UNLOCK(tiny_mag_ptr);
	}

	CHECK(szone, __PRETTY_FUNCTION__);
}
```



신기한 건 checksum 함수가 이 것 밖에 없는 듯 하다 (다른건 잘 모르겠다..) rack->cookie 값이 중요한데 해당 값은 free 시에 rack 최상위 4bit에 0x0~0xf 값이 설정되는데 fd or bk를 overwrite해서 다른 ptr에 malloc을 할 때 cookie값을 동일하게 맞춰야 하기 때문에 4bit brute force가 필요하다.

```c
static MALLOC_INLINE uintptr_t
free_list_checksum_ptr(rack_t *rack, void *ptr)
{
    uintptr_t p = (uintptr_t)ptr;
    return (p >> NYBBLE) | ((free_list_gen_checksum(p ^ rack->cookie) & (uintptr_t)0xF) << ANTI_NYBBLE); // compiles to rotate instruction
}
```



test code 이다.

```c
#include <stdio.h>
#include <unistd.h>
#include <stdlib.h>
#include <string.h>

int main() {
    char aa[16];
    read(0,aa,0x10); <- debug point...
    int *a = malloc(0x40);
    int *b = malloc(0x40);
    int *c = malloc(0x40);
    int *d = malloc(0x40);
    int *e = malloc(0x40);
    int *f = malloc(0x40);
    memset(a,0x31,1);
    memset(b,0x32,1);
    memset(c,0x33,1);
    memset(d,0x34,1);
    memset(e,0x35,1);
    memset(f,0x36,1);
    free(a);
    free(c);
    free(e);
    int *re1 = malloc(0x40);
    int *re2 = malloc(0x40);
    int *re3 = malloc(0x40);
}
```



6 malloc시 이렇게 할당이 된다. linux chunk구조 와는 다르게 Header가 존재하지 않는다

```c
(lldbinit) x/52gx 0x00007fe871d00000
0x7fe871d00000: 0x0000000000000031 0x0000000000000000 <- a
0x7fe871d00010: 0x0000000000000000 0x0000000000000000
0x7fe871d00020: 0x0000000000000000 0x0000000000000000
0x7fe871d00030: 0x0000000000000000 0x0000000000000000
0x7fe871d00040: 0x0000000000000032 0x0000000000000000 <- b
0x7fe871d00050: 0x0000000000000000 0x0000000000000000
0x7fe871d00060: 0x0000000000000000 0x0000000000000000
0x7fe871d00070: 0x0000000000000000 0x0000000000000000
0x7fe871d00080: 0x0000000000000033 0x0000000000000000 <- c
0x7fe871d00090: 0x0000000000000000 0x0000000000000000
0x7fe871d000a0: 0x0000000000000000 0x0000000000000000
0x7fe871d000b0: 0x0000000000000000 0x0000000000000000
0x7fe871d000c0: 0x0000000000000034 0x0000000000000000 <- d
0x7fe871d000d0: 0x0000000000000000 0x0000000000000000
0x7fe871d000e0: 0x0000000000000000 0x0000000000000000
0x7fe871d000f0: 0x0000000000000000 0x0000000000000000
0x7fe871d00100: 0x0000000000000035 0x0000000000000000 <- e
0x7fe871d00110: 0x0000000000000000 0x0000000000000000
0x7fe871d00120: 0x0000000000000000 0x0000000000000000
0x7fe871d00130: 0x0000000000000000 0x0000000000000000
0x7fe871d00140: 0x0000000000000036 0x0000000000000000 <- f
0x7fe871d00150: 0x0000000000000000 0x0000000000000000
0x7fe871d00160: 0x0000000000000000 0x0000000000000000
0x7fe871d00170: 0x0000000000000000 0x0000000000000000
```



a free 후에 아무런 메타데이터가 들어가있지 않다. 아마 cache상에만 들어가 있고 후에 통합이 되어야 메타데이터가 기록되는듯 하다.

```c
(lldbinit) x/52gx 0x00007fe871d00000
0x7fe871d00000: 0x0000000000000031 0x0000000000000000
0x7fe871d00010: 0x0000000000000000 0x0000000000000000
0x7fe871d00020: 0x0000000000000000 0x0000000000000000
0x7fe871d00030: 0x0000000000000000 0x0000000000000000
```



c free 후에 size 및 cookie가 설정 되어 있는 모습 그리고 rack들에 메타데이터가 저장될 때는 original value에 shift 연산을 통해서 저장한다.

ex. 0x04 << 4 = 0x40 (size)

```c
(lldbinit) x/52gx 0x00007fe871d00000
0x7fe871d00000: 0xd000000000000000 0xd000000000000000 <- a
0x7fe871d00010: 0x0000000000000004 0x0000000000000000
0x7fe871d00020: 0x0000000000000000 0x0000000000000000
0x7fe871d00030: 0x0000000000000000 0x0004000000000000
0x7fe871d00040: 0x0000000000000032 0x0000000000000000 <- b
0x7fe871d00050: 0x0000000000000000 0x0000000000000000
0x7fe871d00060: 0x0000000000000000 0x0000000000000000
0x7fe871d00070: 0x0000000000000000 0x0000000000000000
0x7fe871d00080: 0x0000000000000033 0x0000000000000000 <- c
0x7fe871d00090: 0x0000000000000000 0x0000000000000000
0x7fe871d000a0: 0x0000000000000000 0x0000000000000000
0x7fe871d000b0: 0x0000000000000000 0x0000000000000000
0x7fe871d000c0: 0x0000000000000034 0x0000000000000000 <- d
0x7fe871d000d0: 0x0000000000000000 0x0000000000000000
0x7fe871d000e0: 0x0000000000000000 0x0000000000000000
0x7fe871d000f0: 0x0000000000000000 0x0000000000000000
0x7fe871d00100: 0x0000000000000035 0x0000000000000000 <- e
0x7fe871d00110: 0x0000000000000000 0x0000000000000000
0x7fe871d00120: 0x0000000000000000 0x0000000000000000
0x7fe871d00130: 0x0000000000000000 0x0000000000000000
0x7fe871d00140: 0x0000000000000036 0x0000000000000000 <- f
0x7fe871d00150: 0x0000000000000000 0x0000000000000000
0x7fe871d00160: 0x0000000000000000 0x0000000000000000
0x7fe871d00170: 0x0000000000000000 0x0000000000000000
```



e free 후

a, c 에 fd, bk와 같은 포인터들이 설정된다 마찬가지로 shift 연산 되어 있다. 그리고 해당 ptr을 변조할 때 최상위 4bit를 맞춰 주어야한다. 0x0000000000000004, 0x0004000000000000 이렇게 size 같아보이는 녀석들이 존재하는데 아마 fd, bk의 size를 명시하는 것이 아닐까 싶다. 해당 벡터들을 조작해서 통합 시키면 free list block size가 조작되고 조작된 size만큼 사용할 수 있을듯

```c
(lldbinit) x/52gx 0x00007fe871d00000
0x7fe871d00000: 0xf00007fe871d0008[prev] 0xd000000000000000 <- a
0x7fe871d00010: 0x0000000000000004 0x0000000000000000
0x7fe871d00020: 0x0000000000000000 0x0000000000000000
0x7fe871d00030: 0x0000000000000000 0x0004000000000000
0x7fe871d00040: 0x0000000000000032 0x0000000000000000 <- b
0x7fe871d00050: 0x0000000000000000 0x0000000000000000
0x7fe871d00060: 0x0000000000000000 0x0000000000000000
0x7fe871d00070: 0x0000000000000000 0x0000000000000000
0x7fe871d00080: 0xd000000000000000 0xf00007fe871d0000[next] <- c
0x7fe871d00090: 0x0000000000000004 0x0000000000000000
0x7fe871d000a0: 0x0000000000000000 0x0000000000000000
0x7fe871d000b0: 0x0000000000000000 0x0004000000000000
```



ex. 0x50(free) -> 0x40(malloc) 시에 해당 free된 영역에 그대로 들어가긴 하지만 size에 맞춰서 prev ptr, next ptr이 재설정 된다. 

```c
(lldbinit) x/32gx 0x00007f968ad00000
0x7f968ad00000: 0x7000000000000032 0x7000000000000000
0x7f968ad00010: 0x0000000000000005 0x0000000000000000
0x7f968ad00020: 0x0000000000000000 0x0000000000000000
0x7f968ad00030: 0x0000000000000000 0x0000000000000000
0x7f968ad00040: 0x7000000000000000 0x600007f968ad000e
0x7f968ad00050: 0x0000000000000032 0x0000000000000000
0x7f968ad00060: 0x0000000000000000 0x0000000000000000
0x7f968ad00070: 0x0000000000000000 0x0000000000000000
0x7f968ad00080: 0x0000000000000000 0x0000000000000000
0x7f968ad00090: 0x0000000000000000 0x0000000000000000
0x7f968ad000a0: 0x7000000000000031 0x600007f968ad0000
0x7f968ad000b0: 0x0000000000000005 0x0000000000000000
0x7f968ad000c0: 0x0000000000000000 0x0000000000000000
0x7f968ad000d0: 0x0000000000000000 0x0000000000000000
0x7f968ad000e0: 0x600007f968ad0004 0x7000000000000000
0x7f968ad000f0: 0x0000000000000034 0x0000000000000000
```

그리고 해당 영역들이 통합되어야 ptr들이 설정 되는 것 같고 free list block size또한 통합된 크기로 설정되는 것 같다.

그 외에 rack, magazine, region, quantum 같은 용어가 있는데 정확하게는 잘 모르겠다.

rack같은 경우는 chunk, magazine은 free list, cache를 관리하는듯 하고, region이 chunk 인거 같기도 하고.. , quantum 은 memalign byte를 말하는것 같은데 tiny, small, large 마다 크기가 다른 듯..? 요런식으로 이해 하고 있긴 하다..

해당 구조들은 다음과 같다.

```c
# magazine_zone.h -> szone_s

typedef struct szone_s {      // vm_allocate()'d, so page-aligned to begin with.
    malloc_zone_t basic_zone; // first page will be given read-only protection
    uint8_t pad[PAGE_MAX_SIZE - sizeof(malloc_zone_t)];

    unsigned long cpu_id_key; // unused
    // remainder of structure is R/W (contains no function pointers)
    unsigned debug_flags;
    void *log_address;

    /* Allocation racks per allocator type. */
    struct rack_s tiny_rack;
    struct rack_s small_rack;

    /* large objects: all the rest */
    _malloc_lock_s large_szone_lock MALLOC_CACHE_ALIGN; // One customer at a time for large
    unsigned num_large_objects_in_use;
    unsigned num_large_entries;
    large_entry_t *large_entries; // hashed by location; null entries don't count
    size_t num_bytes_in_large_objects;

#if CONFIG_LARGE_CACHE
    int large_entry_cache_oldest;
    int large_entry_cache_newest;
    large_entry_t large_entry_cache[LARGE_ENTRY_CACHE_SIZE]; // "death row" for large malloc/free
    boolean_t large_legacy_reset_mprotect;
    size_t large_entry_cache_reserve_bytes;
    size_t large_entry_cache_reserve_limit;
    size_t large_entry_cache_bytes; // total size of death row, bytes
#endif

    /* flag and limits pertaining to altered malloc behavior for systems with
     * large amounts of physical memory */
    unsigned is_largemem;
    unsigned large_threshold;
    unsigned vm_copy_threshold;

    /* security cookie */
    uintptr_t cookie;

    /* The purgeable zone constructed by create_purgeable_zone() would like to hand off tiny and small
     * allocations to the default scalable zone. Record the latter as the "helper" zone here. */
    struct szone_s *helper_zone;

    boolean_t flotsam_enabled;
} szone_t;
```

```c
# magazine_rack.h -> rack_t

typedef struct rack_s {
    /* Regions for tiny objects */
    _malloc_lock_s region_lock MALLOC_CACHE_ALIGN;

    rack_type_t type;
    size_t num_regions;
    size_t num_regions_dealloc;
    region_hash_generation_t *region_generation;
    region_hash_generation_t rg[2];
    region_t initial_regions[INITIAL_NUM_REGIONS];

    int num_magazines;
    unsigned num_magazines_mask;
    int num_magazines_mask_shift;
    uint32_t debug_flags;

    // array of per-processor magazines
    magazine_t *magazines;

    uintptr_t cookie;
    uintptr_t last_madvise;
} rack_t;
```

```c
# magazine_zone.h -> magazine_t

typedef struct magazine_s { // vm_allocate()'d, so the array of magazines is page-aligned to begin with.
    // Take magazine_lock first,  Depot lock when needed for recirc, then szone->{tiny,small}_regions_lock when needed for alloc
    _malloc_lock_s magazine_lock MALLOC_CACHE_ALIGN;
    // Protection for the crtical section that does allocate_pages outside the magazine_lock
    volatile boolean_t alloc_underway;

    // One element deep "death row", optimizes malloc/free/malloc for identical size.
    void *mag_last_free;
    msize_t mag_last_free_msize;    // msize for mag_last_free
#if MALLOC_TARGET_64BIT
    uint32_t _pad;
#endif
    region_t mag_last_free_rgn; // holds the region for mag_last_free

    free_list_t mag_free_list[MAGAZINE_FREELIST_SLOTS];
    uint32_t mag_bitmap[MAGAZINE_FREELIST_BITMAP_WORDS];

    // the first and last free region in the last block are treated as big blocks in use that are not accounted for
    size_t mag_bytes_free_at_end;
    size_t mag_bytes_free_at_start;
    region_t mag_last_region; // Valid iff mag_bytes_free_at_end || mag_bytes_free_at_start > 0

    // bean counting ...
    size_t mag_num_bytes_in_objects;
    size_t num_bytes_in_magazine;
    unsigned mag_num_objects;

    // recirculation list -- invariant: all regions owned by this magazine that meet the emptiness criteria
    // are located nearer to the head of the list than any region that doesn't satisfy that criteria.
    // Doubly linked list for efficient extraction.
    unsigned recirculation_entries;
    region_trailer_t *firstNode;
    region_trailer_t *lastNode;

#if MALLOC_TARGET_64BIT
    uintptr_t pad[320 - 14 - MAGAZINE_FREELIST_SLOTS -
            (MAGAZINE_FREELIST_BITMAP_WORDS + 1) / 2];
#else
    uintptr_t pad[320 - 16 - MAGAZINE_FREELIST_SLOTS -
            MAGAZINE_FREELIST_BITMAP_WORDS];
#endif

} magazine_t;
```

```c
# magazine_zone.h -> tiny_region_t

/*
 * Layout of a tiny region
 */
typedef uint32_t tiny_block_t[4]; // assert(TINY_QUANTUM == sizeof(tiny_block_t))

typedef struct tiny_header_inuse_pair {
    uint32_t header;
    uint32_t inuse;
} tiny_header_inuse_pair_t;

typedef struct region_trailer {
    struct region_trailer *prev;
    struct region_trailer *next;
    boolean_t recirc_suitable;
    volatile int pinned_to_depot;
    unsigned bytes_used;
    mag_index_t mag_index;
} region_trailer_t;

#define NUM_TINY_BLOCKS 64520

typedef struct tiny_region {
    tiny_block_t blocks[NUM_TINY_BLOCKS];

    region_trailer_t trailer;

    // The interleaved bit arrays comprising the header and inuse bitfields.
    // The unused bits of each component in the last pair will be initialized to sentinel values.
    tiny_header_inuse_pair_t pairs[CEIL_NUM_TINY_BLOCKS_WORDS];

    uint8_t pad[TINY_REGION_SIZE - (NUM_TINY_BLOCKS * sizeof(tiny_block_t)) - TINY_METADATA_SIZE];
} * tiny_region_t;
```



### Env variable

특정 환경 변수들을 설정해서 메모리 할당 기능을 제어할 수 있는 듯 하다.

이 환경 변수들은 MallocHelp 변수를 호출 한 다음 Malloc() 함수를 호출 한다. 

overflow에 가장 유용한 변수들을 살펴보자

##### MallocStackLogging

- 이 변수가 설정되면 레코드가 유지됨
- 모든 malloc을 조작할 수 있고 이 변수를 이용해서 참조되지 않는 malloc()의 버퍼에 대한 프로세스 메모리를 leak할 수 있다.

##### MallocStackLoggingNoCompact

- 이 변수가 설정되면 malloc 작업 기록은 "malloc_history" 가 구문 분석을 할 수 있는 방식으로 유지된다.
- malloc_history는 할당 및 할당 해제를 나열하는 데 사용된다.

##### MallocScribble

- 사용 가능한 블록에 대해 탐지 : 0x55는 free , 0xaa는 allocation으로 판별한다.

##### MallocBadFreeAbort

- 이 변수는 할당된 것으로 나열되지 않은 free()에 포인터가 전달되는 경우 SIGABRT가 프로그램으로 보내지도록 한다.

---

- MallocGuardEdges : True/False
- MallocStackLogging : lite/malloc/vm/vmlite
- MallocStackLoggingNoCompact
- MallocScribble : True/False
- MallocErrorAbort : True/False
- MallocTracing : True/False
- MallocCorruptionAbort : True/False (only 64bit processor)
- MallocCheckHeapStart : -1,0,integer
- MallocCheckHeapEach : -1,0,integer
- MallocCheckHeapAbort : True/False
- MallocCheckHeapSleep : Integer
- MallocMaxMagazines : Integer
- MallocRecircRetainedRegions : Positive Integer
- MallocHelp : True/False

---

Exploit에 관한건 다음 글로..



### Reference

http://blog.shpik.kr/2019/OSX_Heap_Exploitation/

https://www.synacktiv.com/ressources/Sthack_2018_Heapple_Pie.pdf

http://phrack.org/issues/63/5.html

https://opensource.apple.com/source/libmalloc/libmalloc-166.220.1/src/

[http://4ch12dy.site/2019/04/01/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3macos-heap/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3macos-heap/](http://4ch12dy.site/2019/04/01/深入理解macos-heap/深入理解macos-heap/)

https://papago.naver.com/ 